---
title: Implementing a Feedforward Neural Network (FNN) in TensorFlow.js
description: |
  Learn how to build a Feedforward Neural Network (FNN) using TensorFlow.js, focusing on defining the model architecture, training it, and making predictions.
publishDate: 2024-11-21
heroImage: '../../content/post/_images/2022-02-16-example-article-1/focus6.png'
heroAlt: Basic Linear Regression in TensorFlow.js.
tags:
  - tensorflow.js
  - machine learning
  - javascript
category: tutorials
toc: true
---

import { Icon } from 'astro-icon/components';
import { Image } from 'astro:assets';

import Alert from '../../components/Alert.astro';
import Button from '../../components/Button.astro';

## Building a Feedforward Neural Network (FNN) in TensorFlow.js

A Feedforward Neural Network (FNN) is one of the most fundamental neural network architectures. It processes data in one direction—from input to output—and is commonly used for tasks like regression and classification. TensorFlow.js makes it easy to create and train FNNs directly in JavaScript for both browser and Node.js applications.

### Key Steps in Building an FNN

1. **Define the Model Architecture**
2. **Compile the Model**
3. **Prepare the Data**
4. **Train the Model**
5. **Evaluate the Model**
6. **Make Predictions**

### Example: Building an FNN for Binary Classification

This example demonstrates how to build an FNN to predict trading outcomes (`win` or `lose`) based on the provided trading data.

```ts title="Feedforward Neural Network in TensorFlow.js"
import * as tf from '@tensorflow/tfjs';

// Step 1: Define the Model Architecture
const model = tf.sequential();
model.add(tf.layers.dense({ units: 16, activation: 'relu', inputShape: [5] })); // Hidden layer with 16 neurons
model.add(tf.layers.dense({ units: 8, activation: 'relu' }));                  // Second hidden layer
model.add(tf.layers.dense({ units: 1, activation: 'sigmoid' }));               // Output layer for binary classification

// Step 2: Compile the Model
model.compile({
  optimizer: 'adam', // Adaptive optimizer
  loss: 'binaryCrossentropy', // Loss function for binary classification
  metrics: ['accuracy'],      // Performance metric
});

// Step 3: Prepare the Data
const rawData = [
  { features: [0.24, 0.12, 1.68, -0.08, 7], label: 1 }, // Example: Features from dataset and label (win)
  { features: [0.05, 0.18, 1.52, -0.1, 7], label: 1 },
  { features: [0.02, 0.47, -5.62, -0.1, 7], label: 0 }, // Label (lose)
];

// Convert raw data into tensors
const xs = tf.tensor2d(rawData.map(d => d.features), [rawData.length, 5]); // 5 features per sample
const ys = tf.tensor2d(rawData.map(d => d.label), [rawData.length, 1]);   // Binary labels

// Step 4: Train the Model
(async () => {
  await model.fit(xs, ys, {
    epochs: 50, // Number of iterations
    batchSize: 4, // Number of samples per batch
    verbose: 1,  // Display training logs
  });

  // Step 5: Make Predictions
  const testSample = tf.tensor2d([[0.14, 0.08, 5.04, -0.12, 7]], [1, 5]); // Example test data
  const prediction = model.predict(testSample);
  prediction.print(); // Outputs a probability between 0 and 1
})();
```

## Key Concepts to Remember

- **Feedforward Architecture**:
  - Information flows in one direction (input → hidden → output).

- **Activation Functions**:
  - `relu`: Introduces non-linearity in hidden layers.
  - `sigmoid`: Outputs probabilities in the range [0, 1].

- **Loss Function**:
  - `binaryCrossentropy`: Measures how well the model predicts binary outcomes.

- **Optimizer**:
  - `adam`: Adapts the learning rate for efficient weight updates.
